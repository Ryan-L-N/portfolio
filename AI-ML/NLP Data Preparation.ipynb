{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpkDf8Ca0wq5"
   },
   "source": [
    "# NLP Data Preparation\n",
    "\n",
    "With the growth of large-scale natural language processing systems like ChatGPT, more and more groups are trying to build their own natural language processing systems for personal use. As a machine learning engineer, it is critical to not only be able to perform advanced modelling tasks, but to also critically assess the datasets that you use, especially if they rely on using data from several different domains interchangeably.\n",
    "\n",
    "In this project, we shall extract our own COVID-19 dataset from three separate sources (Twitter/X Data, News Article Data, and Research Paper Data ), and attempt to use data engineering to reduce the [Proxy A-Distance](https://papers.nips.cc/paper_files/paper/2006/hash/b1b0432ceafb0ce714426e9114852ac7-Abstract.html) between them. Reducing this distance should allow most systems trained on this data to focus on the content of the data, which we care about, instead of caring about the source of the data, which we do not care about. In doing so, we will demonstrate the importance of feature extraction when it comes to natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWjhBRVv0wrB"
   },
   "source": [
    "### Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "PcJqDhhX0wrC"
   },
   "outputs": [],
   "source": [
    "import collections, os, re\n",
    "import pickle\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pdfminer import high_level\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell has been tagged with excluded_from_script\n",
    "# it will not be run by the autograder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiwO3ebC0wrI"
   },
   "source": [
    "## Part A: Social Media Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ctwt4mb0wrI"
   },
   "source": [
    "While it's current popularity has been waning, Twitter/X was one of the most popular social media platforms; according to [Statista](https://www.statista.com/statistics/272014/global-social-networks-ranked-by-number-of-users/), it has 556 million monthly active users. As tweets/posts are public information, they can provide us important hour by hour information for how the country felt during lockdown situations.\n",
    "\n",
    "In this section, you will perform basic preprocessing and feature extraction on tweet data. As we talked about in the primer, the goal with natural language processing is to convert sequences of text into vectors we can then use in any machine learning algorithm. As our goal is to ensure all three sources of data are undistinguishable by our distance metric, our goal here is to both get a glimpse of the data, and remove any obvious differences that we see.\n",
    "\n",
    "To begin - let's start by loading the twitter response dataset, and looking at a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sseLhnqr_oxp",
    "outputId": "f9fc674f-d7d7-4e81-8831-ad5bb768b498",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Good info from @FlexJobs: Create your own hand sanitizer. The recipe: https://t.co/5GLdNUNmg6 #coronavirus #coronavirusoutbreak #coronaoutbreak',\n",
       "  'lang': 'en',\n",
       "  'id': 101376,\n",
       "  'time': '2020-03-09'},\n",
       " {'text': 'Let√¢‚Ç¨‚Ñ¢s see, during a virus crisis  do I want the guy who√¢‚Ç¨‚Ñ¢s going to battle for Medicare for all or the guy who goes meh adequate is fine??? #CoronavirusOutbreak #COVID19 #Bernie2020',\n",
       "  'lang': 'en',\n",
       "  'id': 76040,\n",
       "  'time': '2020-03-09'},\n",
       " {'text': \"Be carefull guy's and wish you all happy holi to you &amp; your family. :) \\n#HappyHoli #CoronavirusOutbreak #√†¬§¬π√†¬•‚Äπ√†¬§¬≤√†¬•‚Ç¨ #√†¬§¬π√†¬•‚Äπ√†¬§¬≤√†¬§¬ø√†¬§‚Ä¢√†¬§¬æ_√†¬§¬¶√†¬§¬π√†¬§¬® #BankLooteriBJP #Coronavid19 #marketcrash  #reliance #colours #KurkureWithSidNaaz #MondayMorning #MereAngneMein ##RangBarseWithSid #√†¬§¬¨√†¬•ÔøΩ√†¬§¬∞√†¬§¬æ_√†¬§¬®_√†¬§¬Æ√†¬§¬æ√†¬§¬®√†¬•‚Äπ_√†¬§¬π√†¬•‚Äπ√†¬§¬≤√†¬•‚Ç¨_√†¬§¬π√†¬•ÀÜ https://t.co/Rg2SpMNKZD\",\n",
       "  'lang': 'en',\n",
       "  'id': 73533,\n",
       "  'time': '2020-03-09'},\n",
       " {'text': 'Latest Update on #coronavirus √∞≈∏≈íÔøΩ Wide\\n\\n3/9/2020, 6:33:16 AM\\n\\nTotal #Confirmed Cases: 110,034\\nTotal #Deaths: 3,825\\nTotal #Recovered: 61,977\\n\\nSource: Johns Hopkins university Database\\n#Coronavid19 #CoronavirusOutbreak',\n",
       "  'lang': 'en',\n",
       "  'id': 65859,\n",
       "  'time': '2020-03-09'}]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"twitter.txt\",'r') as twitter_dataset_file:\n",
    "    twitter_dataset = json.load(twitter_dataset_file)\n",
    "\n",
    "[twitter_dataset[0], twitter_dataset[1000], twitter_dataset[8200], twitter_dataset[9000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uRjBrQx0wrL"
   },
   "source": [
    "### Question 1: Process tweet data\n",
    "Looking at some of the tweets above, we see that:\n",
    "1. Some tweets contain Twitter-shortened URLs, for example `https://t.co/DzhsXPxUDa`. These are always in the form of `http://t.co/` or `https://t.co/` followed by 10 alphanumeric characters. These links should be removed, as they are unlikely to be in any other data set.\n",
    "1. Some tweets contain emoticons such as `:)` or `<3`. The characters in these emoticons should be removed, as again they are unlikely to be in any other dataset.\n",
    "\n",
    "Implement the function `process_tweet` that takes as the text of a tweet, performs these two steps, removes the whitespace ahead and after the tweet, and then returns the text data altogether.\n",
    "\n",
    "**Notes**:\n",
    "* You should remove URL before removing emoticons.\n",
    "* We have provided a list of emoticons for you in the variable `emoticons`. You can assume that only elements in this set are considered emoticons and need to be removed.\n",
    "* Note that there may be no space between a shortened URL and the next word. However, you can assume that there are always 10 alphanumeric characters after http://t.co/ or https://t.co/.\n",
    "* When you finish processing the text, remember to ensure that all beginning and following whitespace is removed using `.strip()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "8khGD3Wc0wrL"
   },
   "outputs": [],
   "source": [
    "emoticons = [\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3',\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', 'b=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "]\n",
    "\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Process a tweet by:\n",
    "    1. Removing Twitter-shortened URLs (replaced with space)\n",
    "    2. Removing emoticons (replaced with space)\n",
    "    3. Stripping leading and trailing whitespace\n",
    "    \"\"\"\n",
    "    # Step 1: Remove Twitter-shortened URLs (replace with space)\n",
    "    url_pattern = r'https?://t\\.co/[a-zA-Z0-9]{10}'\n",
    "    processed = re.sub(url_pattern, ' ', tweet)\n",
    "    \n",
    "    # Step 2: Remove emoticons (replace with space)\n",
    "    for emoticon in emoticons:\n",
    "        processed = processed.replace(emoticon, ' ')\n",
    "    \n",
    "    # Step 3: Strip leading and trailing whitespace\n",
    "    processed = processed.strip()\n",
    "    \n",
    "    return processed\n",
    "\n",
    "\n",
    "# NOTE: do not modify this function.\n",
    "def process_tweet_data(tweets: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Process a list of tweets by extracting and tokenizing their text.\n",
    "\n",
    "    args:\n",
    "        tweets (list) : a list of tweet dictionaries, each containing a 'text' key\n",
    "\n",
    "    return:\n",
    "        list : a list of processed tweet texts\n",
    "    \"\"\"\n",
    "    return [process_tweet(tweet[\"text\"]) for tweet in tweets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ci83YAp30wrL",
    "outputId": "9b604fda-c920-4c00-b194-f29f92dc8cd0",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_process_tweet() -> None:\n",
    "    \"\"\"\n",
    "    Unit test for the process_tweet function.\n",
    "\n",
    "    This function verifies that process_tweet correctly removes URLs and emoticons\n",
    "    while preserving meaningful text. It uses assertions to compare expected outputs \n",
    "    against actual results.\n",
    "\n",
    "    Test cases:\n",
    "        - Removes emoticons while keeping the text intact\n",
    "        - Removes heart symbols and other inline special characters\n",
    "        - Handles different emoticon variations correctly\n",
    "        - Strips out URLs but preserves punctuation\n",
    "        - Maintains proper spacing when URLs are removed\n",
    "        - Ensures URLs with incorrect formats remain unchanged\n",
    "        - Keeps hashtags and non-URL special characters in tweets\n",
    "\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    assert process_tweet(\"It's a great day :D\") == \"It's a great day\"\n",
    "    assert process_tweet(\"<3hello\") == \"hello\"\n",
    "    assert process_tweet(\"goodX-Dday\") == \"good day\"\n",
    "    assert process_tweet(\"http://t.co/WJs5bmRthU,http://t.co/WJs5bmRthU,\") == \", ,\"\n",
    "    assert process_tweet(\"hellohttp://t.co/WJs5bmRthUworld\") == \"hello world\"\n",
    "    assert process_tweet(\"http://taco/WJs5bmRthU\") == \"http://taco/WJs5bmRthU\"\n",
    "    assert process_tweet(\n",
    "        'Protect your child from #CoronavirusOutbreak.\\n\\nhttps://t.co/qPREVvM2C5\\n\\n#CoronaVirusUpdate #COVID2019 #COVID #Coronavid19 #outbreak #Italy #COVID√£∆í¬º19 #BeSafe #Containment #Homeschooling #DigitalTransformation #InternationalSchooling #virtualschool #OnlineNOW #edtech #technology'\n",
    "    ) == \"Protect your child from #CoronavirusOutbreak.\\n\\n \\n\\n#CoronaVirusUpdate #COVID2019 #COVID #Coronavid19 #outbreak #Italy #COVID√£∆í¬º19 #BeSafe #Containment #Homeschooling #DigitalTransformation #InternationalSchooling #virtualschool #OnlineNOW #edtech #technology\"\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_process_tweet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm1MU9dI0wrL"
   },
   "source": [
    "## Part B: Process Web Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2w-gTJp0wrM"
   },
   "source": [
    "Let's now move to extracting text from web articles, using Beautifulsoup to parse HTML data. More specifically, we have collected news articles related to the same topic of Coronavirus from [Nature](https://www.nature.com/), and want to also wrangle this data accordingly. Through this exercise, you will learn how to navigate HTML structures from different webpages in order to get the desired information.\n",
    "\n",
    "To begin, we have provided you a helper function `retrieve_url` takes as input a webpage string URL and creates a BeautifulSoup object from the corresponding page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "Qe4mtqgh5CxC"
   },
   "outputs": [],
   "source": [
    "def retrieve_url(local_file_location: str) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Read and parse an HTML file to extract its content using BeautifulSoup.\n",
    "\n",
    "    args:\n",
    "        local_file_location (str) : the path to the local HTML file\n",
    "\n",
    "    return:\n",
    "        BeautifulSoup : a parsed BeautifulSoup object representing the HTML content\n",
    "    \"\"\"\n",
    "    with open(local_file_location, \"r\", encoding=\"utf-8\") as file:\n",
    "        html = file.read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R_I88IH0wrM"
   },
   "source": [
    "### Question 2: Parsing a single article from Nature\n",
    "Implement the function `parse_page_nature` that takes as input a path pointing to a text dump of a Nature news article, and returns a JSON dictionary with the following format:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'Title': 'When will the coronavirus outbreak peak?' #str\n",
    "    'Author': ['David Cyranoski'] # list, a list of author names in the same order as they appear on the page\n",
    "    'Published Date': '2020-04-21' # str, yyyy-mm-dd\n",
    "    'Summary': '.....' #str, the summary div between the title and author fields, or empty string if no summary is available\n",
    "    'Content': ['.....'] #Any content that follows the author fields, or empty string if no content is available.\n",
    "}\n",
    "```\n",
    "\n",
    "The values of `Summary` and `Content` should be raw texts that do not contain any HTML tag. For example, if the input HTML code is `\"<p><b>Hello</b><a href=\"https://google.com\">World</a><p>\"` then the output `Content` should be `\"Hello World\"`.\n",
    "\n",
    "In the local test we have provided the full reference JSON files for some article pages. If your dictionary does not match the reference JSON, you should print out both and do a careful comparison to see where the difference is.\n",
    "\n",
    "**Notes**:\n",
    "* Occasionally there are some \"Related\" blocks embedded in the article text (example [here](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-domain-data-preparation/nature_related.png)). These are characterized by the attribute `data-label=\"Related\"` and should **not** be included in the parsing result.\n",
    "* The `Published Date` field should be the original article date, not the updated date. For example, the `Published Date` for [this article](https://www.nature.com/articles/d41586-020-00166-6) is 2020-01-22.\n",
    "* Remember to call `strip()` on all values in the returned dictionary so that there is no leading or trailing space anywhere. If a content paragraph becomes empty after `strip()`, it should not be included. You do not need to call any other text processing task in section A.\n",
    "* Do not parse information form the `meta` tags as they are not robust. Every required information can be found within `body`.\n",
    "* If an article has no authors (e.g., https://www.nature.com/articles/d41586-020-00589-1), the Author field should be an empty list.\n",
    "* For the Content list, only text contents that come from the `p` tags in the article body should be included. You can start by identifying a `div` that corresponds to the entire article body (looking at the CSS class names may be helpful). Note that if an image caption is the child of a `p` tag, its content should be included as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "oFa1fdUx0wrM"
   },
   "outputs": [],
   "source": [
    "def remove_unused_content(soup: BeautifulSoup) -> None:\n",
    "    \"\"\"\n",
    "    Remove specific unused content from a parsed HTML document.\n",
    "\n",
    "    This function searches for elements that contain the attribute 'data-label' \n",
    "    with the value 'Related' and removes them from the BeautifulSoup object.\n",
    "\n",
    "    args:\n",
    "        soup (BeautifulSoup) : a parsed BeautifulSoup object representing the HTML content\n",
    "\n",
    "    return:\n",
    "        None : modifies the soup object in place\n",
    "    \"\"\"\n",
    "    for unused_content in soup.find_all(True, {\"data-label\": \"Related\"}):\n",
    "        unused_content.extract()\n",
    "\n",
    "\n",
    "def parse_page_nature(url: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse a single New York Times article at the given URL.\n",
    "\n",
    "    args:\n",
    "        url (str) : the article URL\n",
    "\n",
    "    return:\n",
    "        Dict[str, str] : the parsed information stored in JSON format, which includes:\n",
    "            Title, Author, Published Date, Summary and Content\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    # Get page content.\n",
    "    soup = retrieve_url(url)\n",
    "    remove_unused_content(soup)\n",
    "\n",
    "    # Add the \"Title\" field to the \"result\" dict.\n",
    "    result[\"Title\"] = soup.find(\"h1\", class_=\"c-article-magazine-title\").get_text().strip()\n",
    "    \n",
    "    # Add the \"Content\" field to the \"result\" dict.\n",
    "    result[\"Content\"] = []\n",
    "    if soup.find(\"p\", class_=\"article__teaser\") is not None:\n",
    "      for p in soup.find(\"p\", class_=\"article__teaser\").find_all(\"p\"):\n",
    "          for child in p.find_all():\n",
    "              child.unwrap()\n",
    "          content = p.get_text().strip()\n",
    "          if len(content) > 0:\n",
    "              result[\"Content\"].append(content)\n",
    "    body = soup.find(\"div\", class_=\"c-article-body\")\n",
    "    if body is not None:\n",
    "      for p in body.find_all(\"p\"):\n",
    "          for child in p.find_all():\n",
    "              child.unwrap()\n",
    "          content = p.get_text().strip()\n",
    "          if len(content) > 0:\n",
    "              result[\"Content\"].append(content)\n",
    "    \n",
    "    # TODO: Add the \"Author\" field to the \"result\" dict.\n",
    "    authors_raw = [tag['content'] for tag in soup.find_all('meta', {'name': 'dc.creator'})]\n",
    "    authors = [' '.join(name.split(', ')[::-1]) for name in authors_raw]\n",
    "    result[\"Author\"] = authors\n",
    " \n",
    "    # TODO: Add the \"Summary\" field to the \"result\" dict.\n",
    "    result[\"Summary\"] = soup.find('div', class_='c-article-teaser-text').get_text(strip=True)\n",
    "    \n",
    "    # TODO: Add the \"Published Date\" field to the \"result\" dict.\n",
    "    result[\"Published Date\"] = soup.find('meta', {'name': 'dc.date'})['content']\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Cr7AZx00wrM",
    "outputId": "dc04d9f9-5e56-43a1-b789-7f1661db87e1",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'China coronavirus: Six questions scientists are asking', 'Author': ['Ewen Callaway', 'David Cyranoski'], 'Summary': 'Researchers are racing to find out more about the epidemiology and genetic sequence of the coronavirus spreading in Asia and beyond.', 'Published Date': '2020-01-22', 'Content': []}\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_parse_page_nature() -> None:\n",
    "    \"\"\"\n",
    "    Unit test for the parse_page_nature function.\n",
    "\n",
    "    This function verifies that parse_page_nature correctly parses HTML files \n",
    "    and extracts the expected content by comparing the output against reference \n",
    "    JSON files.\n",
    "\n",
    "    Test cases:\n",
    "        - Parses different HTML files and compares them with expected reference data\n",
    "        - Ensures extracted content matches predefined JSON structures\n",
    "        - Prints reference data for debugging purposes\n",
    "\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    nature0 = parse_page_nature(\"html_data/nature_0.html\")\n",
    "    nature0_ref = json.load(open(\"local_test_refs/0_ref.txt\"))\n",
    "    print(nature0_ref)\n",
    "    assert nature0 == nature0_ref\n",
    "\n",
    "    nature1 = parse_page_nature(\"html_data/nature_1.html\")\n",
    "    nature1_ref = json.load(open(\"local_test_refs/1_ref.txt\"))\n",
    "    assert nature1 == nature1_ref\n",
    "\n",
    "    nature2 = parse_page_nature(\"html_data/nature_2.html\")\n",
    "    nature2_ref = json.load(open(\"local_test_refs/2_ref.txt\"))\n",
    "    assert nature2 == nature2_ref\n",
    "\n",
    "    nature3 = parse_page_nature(\"html_data/nature_3.html\")\n",
    "    nature3_ref = json.load(open(\"local_test_refs/3_ref.txt\"))\n",
    "    assert nature3 == nature3_ref\n",
    "\n",
    "    nature4 = parse_page_nature(\"html_data/nature_4.html\")\n",
    "    nature4_ref = json.load(open(\"local_test_refs/4_ref.txt\"))\n",
    "    assert nature4 == nature4_ref\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_parse_page_nature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0l7Cqxt0wrR"
   },
   "source": [
    "### Question 3: Process news articles data\n",
    "While the JSON data format we constructed earlier is useful for checking the correctness of our parsing, eventually we would like each article to be represented by just a string. For our purpose, we will define the string representation of an article as\n",
    "\n",
    "`\"<title> <summary> <content paragraph 1> <content paragraph 2> <content paragraph 3> ...\"`\n",
    "\n",
    "where there is a single space separating each field (note that the content paragraphs come from the `\"Content\"` field of an article json, which is a list of paragraph strings).\n",
    "\n",
    "Implement the function `process_news_article` that takes as input a JSON dictionary resulting from parsing a Nature or NYT article, and converts the JSON to the above string format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "fDCKrvuX0wrS"
   },
   "outputs": [],
   "source": [
    "def process_news_article(article: Dict[str, str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert article jsons to nested list of tokens of processed article contents.\n",
    "\n",
    "    args:\n",
    "        article (Dict[str, str]] : JSON content of a news article\n",
    "\n",
    "    return:\n",
    "        List[str] : a list of processed tokens from the input article JSON\n",
    "    \"\"\"\n",
    "    title = article['Title']\n",
    "    summary = article['Summary']\n",
    "    contents = article['Content']\n",
    "    final_string = title + \" \" + summary + \" \" + \" \".join(contents)\n",
    "    return final_string\n",
    "\n",
    "\n",
    "# NOTE: do not modify this function.\n",
    "def process_news_articles_data(articles: List) -> List:\n",
    "    \"\"\"\n",
    "    Process a list of news articles by extracting and formatting their content.\n",
    "\n",
    "    args:\n",
    "        articles (list) : a list of JSON objects, each representing a news article\n",
    "\n",
    "    return:\n",
    "        list : a list of processed news articles\n",
    "    \"\"\"\n",
    "    return [process_news_article(article) for article in articles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjQXmgZg0wrS",
    "outputId": "a3c8dc6a-d436-4555-9811-4db45e156b58",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_process_news_article() -> None:\n",
    "    \"\"\"\n",
    "    Unit test for the process_news_article function.\n",
    "\n",
    "    This function verifies that process_news_article correctly processes a news \n",
    "    article by comparing its output against a predefined expected result.\n",
    "\n",
    "    Test cases:\n",
    "        - Loads a reference news article from a JSON file\n",
    "        - Processes the article and compares it to an expected processed output\n",
    "        - Ensures the processed article matches the expected result\n",
    "\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    nature_article = json.load(open(\"local_test_refs/4_ref.txt\", \"r\", encoding=\"utf-8\"))\n",
    "    nature_article_processed = process_news_article(nature_article)\n",
    "    nature_expected = open(\"local_test_refs/nature4_processed.txt\").read()\n",
    "    assert nature_article_processed == nature_expected\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_process_news_article()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDTiSdam0wrS"
   },
   "source": [
    "## Part C: Mining PDF Data\n",
    "Having extracted data from Twitter and newspapers, we now turn to our third source: research papers. We have provided you with 15 pdf files, collected from the [arxiv API](https://arxiv.org/help/api). These are located in the `pdfs` directory and labeled from `arxiv_01.pdf` to `arxiv_15.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgB7wSyz0wrT"
   },
   "source": [
    "### Question 4: Parse a single Arxiv research paper\n",
    "Implement the function `parse_pdf` that takes as input a PDF file path and outputs the processed tokenization of the text content of that file. In particular, you should remove all URLs, i.e., strings that start with \"http://\" or \"https://\".\n",
    "\n",
    "**Notes**:\n",
    "* For this question, you should use the function [`extract_text`](https://pdfminersix.readthedocs.io/en/latest/reference/highlevel.html#extract-text) from the `pdfminer` package to convert a pdf file to string.\n",
    "* Unlike in the tweet scenario, there is no limit on the length of an URL in this case. The URL pattern you should use here is: a string that starts with `http://` or `https://`, followed by any number of non-space character. Do not make any other assumption (for example, don't assume an URL always contains a `.`).\n",
    "* We have provided a template helper function `remove_url_regex`, where you can enter the regex for removing URLs. There are some local test cases in `test_remove_url_regex` to help you validate your regex. If your regex passes these tests, you can use it in `parse_and_clean_pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "xN22jY990wrT"
   },
   "outputs": [],
   "source": [
    "def get_url_regex_pattern() -> re.Pattern:\n",
    "    \"\"\"\n",
    "    Define and return a regex pattern for capturing URL strings.\n",
    "\n",
    "    This function provides a regular expression that can be used to identify \n",
    "    and remove URLs from text. It can be used within parse_and_clean_pdf() \n",
    "    or other text-processing functions.\n",
    "\n",
    "    return:\n",
    "        re.Pattern : compiled regex pattern for detecting URLs\n",
    "    \"\"\"\n",
    "    url_regex_pattern = re.compile(r\"https?://\\S+\") # TODO: Define the correct regex.\n",
    "    return url_regex_pattern\n",
    "\n",
    "\n",
    "def parse_and_clean_pdf(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert an input pdf file into processed and cleaned raw text.\n",
    "\n",
    "    args:\n",
    "        file_path (str) : the pdf file path\n",
    "\n",
    "    return:\n",
    "        str: the cleaned version of the input file content\n",
    "    \"\"\"\n",
    "    \n",
    "    text = high_level.extract_text(file_path)\n",
    "    pattern = get_url_regex_pattern()\n",
    "    text_cleaned = re.sub(pattern, \"\", text)\n",
    "    return text_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxJ2QWYK0wrT",
    "outputId": "283f1f0d-f779-4b69-91ee-1e9111dc7fc3",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_get_url_regex_pattern() -> None:\n",
    "    \"\"\"\n",
    "    Unit test for the get_url_regex_pattern function.\n",
    "\n",
    "    This function verifies that get_url_regex_pattern correctly identifies and removes \n",
    "    URLs from text using regex substitution.\n",
    "\n",
    "    Test cases:\n",
    "        - Removes a simple HTTP URL completely\n",
    "        - Removes an HTTPS URL embedded within text\n",
    "        - Removes multiple URLs and preserves spacing\n",
    "        - Removes incomplete URLs (e.g., \"https://www.\")\n",
    "\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    url = \"http://abc\"\n",
    "    assert re.sub(get_url_regex_pattern(), \"\", url) == \"\"\n",
    "\n",
    "    url = \"hellohttps://github.com/lanagarmire/COVID19-Drugs-LungInjury\"\n",
    "    assert re.sub(get_url_regex_pattern(), \"\", url) == \"hello\"\n",
    "\n",
    "    url = \"http://example.com https://cmu.edu\"\n",
    "    assert re.sub(get_url_regex_pattern(), \"\", url) == \" \"\n",
    "\n",
    "    url = \"https://www.\"\n",
    "    assert re.sub(get_url_regex_pattern(), \"\", url) == \"\"\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_get_url_regex_pattern()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yJwkmLx0wrT",
    "outputId": "2c1dbb72-bded-4fa2-df79-b922c9693636",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_parse_and_clean_pdf() -> None:\n",
    "    \"\"\"\n",
    "    Unit test for the parse_and_clean_pdf function.\n",
    "\n",
    "    This function verifies that parse_and_clean_pdf correctly processes and cleans\n",
    "    text extracted from a PDF by comparing its output against a predefined expected result.\n",
    "\n",
    "    Test cases:\n",
    "        - Loads a reference PDF and compares the processed text to the expected cleaned text\n",
    "        - Ensures the text extraction and cleaning process works as intended\n",
    "\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    processed_pdf_text = parse_and_clean_pdf(\"pdfs/arxiv_01.pdf\")\n",
    "    with open(\"local_test_refs/parsed_arxiv_01.txt\") as expected_pdf_text:\n",
    "        assert processed_pdf_text == expected_pdf_text.read()\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_parse_and_clean_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRARSOnGGoEv"
   },
   "source": [
    "### Question 5: Parse several Arxiv research papers\n",
    "Implement the function `process_arxiv_data` that takes as input the path to a directory. This function parses and cleans all pdf files in that directory, then returns a list of strings, where each string results from parsing one PDF file.\n",
    "\n",
    "**Hint**: You might find using your previous question useful.\n",
    "\n",
    "**Notes**:\n",
    "* The pdf files should be processed based on the alphabetical order of their name, e.g., `arxiv_01.pdf` before `arxiv_02.pdf`.\n",
    "* Do not assume that `os.listdir` will return the filenames in sorted order; you should perform the sorting yourself.\n",
    "* Do not assume every file in the input directory is a pdf file; only those whose names end in `.pdf` should be parsed.\n",
    "* If you fail the test case here, it is likely that your URL removal regex from Question 10 is incorrect. Try to come up with more test cases to test your URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "lKoyv5xcGmO2"
   },
   "outputs": [],
   "source": [
    "def process_arxiv_data(directory: str, num_files: int = 15) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Parse and process the text content of all pdf papers in alphabetical order in a given directory.\n",
    "\n",
    "    args:\n",
    "        directory (str) : the relative file path to a directory that contains the pdf papers\n",
    "        num_files (int) : Only return the first num_files files\n",
    "\n",
    "    return:\n",
    "        List[List[str]] : a list of list of word tokens\n",
    "    \"\"\"\n",
    "    files = sorted(os.listdir(directory))\n",
    "    list_of_strings = []\n",
    "    for index, file in enumerate(files):\n",
    "        if index < num_files:\n",
    "            text = parse_and_clean_pdf(directory + \"/\" + file)\n",
    "            list_of_strings.append(text)      \n",
    "    return list_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3DE3lE9GsQU",
    "outputId": "afc708ab-a371-46a9-dc38-e80bae412cec",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_process_arxiv_data() -> None:\n",
    "    \"\"\"\n",
    "    Unit test for the process_arxiv_data function.\n",
    "\n",
    "    This function verifies that process_arxiv_data correctly processes a directory of PDFs \n",
    "    and extracts their contents by comparing the extracted text with expected results.\n",
    "\n",
    "    Test cases:\n",
    "        - Ensures the correct number of papers are processed (15 in this case)\n",
    "        - Verifies that a sample of the extracted contents matches predefined expected values\n",
    "        - Checks that the total length of the extracted text matches the expected total\n",
    "\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    paper_contents = process_arxiv_data(\"pdfs\")\n",
    "    sample_of_contents = [paper[0:100] for paper in paper_contents]\n",
    "\n",
    "    assert len(paper_contents) == 15\n",
    "    assert sample_of_contents == ['Repurposed drugs for treating lung injury in COVID-19 \\n\\nBing He1, Lana Garmire1* \\n\\n1. Department of ', '0\\n2\\n0\\n2\\n\\nr\\na\\n\\nM\\n1\\n3\\n\\n]\\nE\\nP\\n.\\no\\ni\\nb\\n-\\nq\\n[\\n\\n1\\nv\\n4\\n8\\n2\\n4\\n1\\n.\\n3\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nThe fractal time grow', 'Coronavirus and financial volatility: 40 days of fasting and fear \\n\\nClaudiu Tiberiu ALBULESCU1,2\\uf02a \\n\\n', '0\\n2\\n0\\n2\\n\\nr\\na\\n\\nM\\n8\\n\\n]\\nE\\nP\\n.\\no\\ni\\nb\\n-\\nq\\n[\\n\\n1\\nv\\n5\\n7\\n7\\n3\\n0\\n.\\n3\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nData Analysis for the C', 'How  many  infections  of  COVID-19  there  will  be  in  the  ‚ÄúDiamond  Princess‚Äù-\\n\\nPredicted by a ', 'Parametric analysis of early data on COVID-19 expansion in selected\\nEuropean countries\\n\\naInstitute o', '0\\n2\\n0\\n2\\n\\nr\\na\\n\\nM\\n5\\n2\\n\\n]\\n\\nR\\n\\nI\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n7\\n0\\n1\\n0\\n0\\n.\\n3\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nViewing the Progression o', 'Insights from early mathematical models of 2019-nCoV acute respiratory disease (COVID-\\n\\nEarly models', '0\\n2\\n0\\n2\\n\\nb\\ne\\nF\\n4\\n2\\n\\n]\\nh\\np\\n-\\nc\\no\\ns\\n.\\ns\\nc\\ni\\ns\\ny\\nh\\np\\n[\\n\\n1\\nv\\n2\\n0\\n3\\n0\\n1\\n.\\n2\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nThe Recons', 'COVID-19  Docking  Server:  An  interactive  server  for \\n\\ndocking  small  molecules,  peptides  and', '0\\n2\\n0\\n2\\n\\nb\\ne\\nF\\n2\\n2\\n\\n]\\nE\\nP\\n.\\no\\ni\\nb\\n-\\nq\\n[\\n\\n1\\nv\\n0\\n4\\n6\\n9\\n0\\n.\\n2\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nThe Outbreak Evaluatio', '0\\n2\\n0\\n2\\n\\nb\\ne\\nF\\n5\\n2\\n\\n]\\nh\\np\\n-\\nc\\no\\ns\\n.\\ns\\nc\\ni\\ns\\ny\\nh\\np\\n[\\n\\n2\\nv\\n9\\n9\\n1\\n9\\n0\\n.\\n2\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nScaling fe', '0\\n2\\n0\\n2\\n\\nb\\ne\\nF\\n2\\n1\\n\\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n4\\n3\\n5\\n5\\n0\\n.\\n2\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nABNORMAL RESPIRATORY PATTER', 'Trend and forecasting of the COVID-19 outbreak in\\nChina\\n\\nQiang Li1 Wei Feng2\\n\\n1School of Physical sc', 'Deep  Learning  System  to  Screen  Coronavirus  Disease  2019 \\n\\nPneumonia \\n\\nXiaowei Xu1, MD; Xianga']\n",
    "    assert sum(len(paper) for paper in paper_contents) == 368635\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_process_arxiv_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp2M2EGl0wrD"
   },
   "source": [
    "## Part D: Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72Kpyo_I0wrD"
   },
   "source": [
    "Now that we've converted our initial set of structured data into pure text, let's process all of the text in a similar way. Text data on the internet is very messy.  Typically there is a fair amount of processing work to do once you have collected any sizeable chunk of text data, in order to have it ready for subsequent analyses. To get you familiar with this kind of data, this section will walk you through some common processing tasks:\n",
    "\n",
    "The first step is to import the lemmatizer and set of English stopwords from `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "EzYSYJPT0wrE"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "english_stopwords = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGdvHTIo0wrF"
   },
   "source": [
    "### Question 6: Text cleaning and tokenization\n",
    "Implement the three functions `clean_string`, `tokenize` and `lemmatize` that perform the following text preprocessing tasks:\n",
    "\n",
    "1. `clean_text` should:\n",
    "    * convert the string to lower case.\n",
    "    * remove any instance of `'s` that is either followed by any whitespace character, or at the end of the string: `teacher's help` becomes `teacher help`, and `children's` becomes `children`.\n",
    "    * remove apostrophe character `'`: `don't` becomes `dont`. For simplicity we will only consider the character `'` as apostrophe (so `‚Äô` is not).\n",
    "    * remove leading and trailing space.\n",
    "\n",
    "1. `tokenize` should:\n",
    "    * use `nltk.word_tokenize` to tokenize the input text.\n",
    "    * further break tokens at characters which are not digits 0-9 and not present in `string.ascii_letters`. For example, `a_b_c` becomes `['a', 'b', 'c']`.\n",
    "    * maintain the token order as it appears in the original string.\n",
    "\n",
    "1. `lemmatize` should:\n",
    "    * lemmatize each token individually.\n",
    "    * remove tokens that are stopwords or contain fewer than two characters (these two cases should be checked after the lemmatization step).\n",
    "    \n",
    "**Notes**:\n",
    "* When lemmatizing a word, you should also specify the part-of-speech `pos` parameter. This can be obtained by calling `nltk.pos_tag` and using the first returned tag (in case there are multiple possibilities). You can interpret the returned tag as follows:\n",
    "    * If it starts with \"J\", it is an adjective.\n",
    "    * If it starts with \"V\", it is a verb.\n",
    "    * If it starts with \"R\", it is an adverb.\n",
    "    * Otherwise, it is a noun.\n",
    "* `nltk.pos_tag` should be called on each individual token, instead of on the entire tokenized text. For example, if the input string is `\"learning is fun\"`, you should call `nltk.pos_tag([\"learning\"])` to get the part-of-speech of `'learning'`, and input that to the lemmatizer. You may notice that in this case `\"learning\"` is classified as a verb (while it is a noun in the original sentence). However, this is not a problem, since our end goal is to reduce each token to its base form, not to correctly classify its part-of-speech.\n",
    "* If you use the regex character set `\\w`, note that it matches alphanumeric characters **and** the underscore character `_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "tMONw0Qd0wrG"
   },
   "outputs": [],
   "source": [
    "# NOTE: do not modify this function.\n",
    "def get_pos(word) -> str:\n",
    "    \"\"\"\n",
    "    Determine the WordNet part of speech tag for a given word.\n",
    "\n",
    "    This function takes a word as input, tags it with a part of speech using NLTK's pos_tag, \n",
    "    and then maps the tag to WordNet's part of speech notation.\n",
    "\n",
    "    args:\n",
    "        word (str) : a word to determine the part of speech for\n",
    "\n",
    "    return:\n",
    "        str : WordNet POS tag corresponding to the word's part of speech\n",
    "    \"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1]\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    if tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    if tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input string by converting it to lowercase, removing 's and apostrophe.\n",
    "\n",
    "    args:\n",
    "        text (str) : the input text\n",
    "\n",
    "    return:\n",
    "        str : the cleaned text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    pattern1 = re.compile(r\"'s(?=\\s|$)\")\n",
    "    pattern2 = re.compile(r\"'\")\n",
    "    cleaned_text = re.sub(pattern1, \"\", text)\n",
    "    cleaned_text = re.sub(pattern2, \"\", cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(cleaned_text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize the input string.\n",
    "\n",
    "    args:\n",
    "        cleaned_text (str): the input text, output from clean_text\n",
    "\n",
    "    return:\n",
    "        List[str] : a list of tokens from the input text\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(cleaned_text)\n",
    "    \n",
    "    # Define pattern: split on anything that's not a letter or digit\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9]+')\n",
    "    \n",
    "    # Further split each token and flatten the result\n",
    "    final_tokens = []\n",
    "    for token in tokens:\n",
    "        # Split token on non-alphanumeric characters\n",
    "        sub_tokens = pattern.split(token)\n",
    "        # Filter out empty strings and add to final list -- NOTE: \"if tok\" filters out empty strings\n",
    "        final_tokens.extend([tok for tok in sub_tokens if tok])\n",
    "    \n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "def lemmatize(tokens: List[str], stopwords: Set[str] = {}) -> List[str]:\n",
    "    \"\"\"\n",
    "    Lemmatize each token in an input list of tokens\n",
    "\n",
    "    args:\n",
    "        tokens (List[str]) : a list of token, output from tokenize\n",
    "\n",
    "    kwargs:\n",
    "        stopwords (Set[str]) : the set of stopwords to exclude\n",
    "\n",
    "    return:\n",
    "        List[str] : a list of lemmatized and filtered tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Get WordNet POS tag\n",
    "        pos = get_pos(token)\n",
    "        \n",
    "        # Lemmatize with POS\n",
    "        lemma = lemmatizer.lemmatize(token, pos=pos)\n",
    "        \n",
    "        # Filter: not a stopword AND at least 2 characters\n",
    "        if lemma not in stopwords and len(lemma) >= 2:\n",
    "            lemmatized_tokens.append(lemma)\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# NOTE: do not modify this function.\n",
    "def preprocess_text(text, stopwords = {}):\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = tokenize(cleaned_text)\n",
    "    return lemmatize(tokens, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "394jTGgx0wrG",
    "outputId": "703173a7-6534-4bc6-8fc1-08498a888afb",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_preprocess_text() -> None:\n",
    "    \"\"\"\n",
    "    Unit test for the text preprocessing functions.\n",
    "\n",
    "    This function tests various aspects of text preprocessing, including cleaning, tokenization, \n",
    "    and lemmatization, ensuring that the functions behave as expected for a range of input cases.\n",
    "\n",
    "    Test cases:\n",
    "        - Verifies that the clean_text function processes input as expected (e.g., handling contractions and special characters)\n",
    "        - Confirms the tokenization function splits strings into expected tokens\n",
    "        - Validates that lemmatization handles a variety of words correctly\n",
    "        - Ensures that stop words are correctly removed during preprocessing\n",
    "\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Clean text.\n",
    "    assert clean_text(\"I like Data Science\") == \"i like data science\"\n",
    "    assert clean_text(\"She's\") == \"she\"\n",
    "    assert clean_text(\"you've\")== \"youve\"\n",
    "    assert clean_text(\"car, cars, car's cars'\")== \"car, cars, car cars\"\n",
    "    assert clean_text(\"'shed'\") == \"shed\"\n",
    "    assert clean_text(\"'good news'\") == \"good news\"\n",
    "    assert clean_text(\"CMU's campus\")== \"cmu campus\"\n",
    "    assert preprocess_text(\"abc 'system\") == ['abc', 'system']\n",
    "    assert preprocess_text(\"O'Shea Jackson Jr. is an American actor and musician\") == ['oshea', 'jackson', 'jr', 'be', 'an', 'american', 'actor', 'and', 'musician']\n",
    "\n",
    "    # Tokenize text.\n",
    "    assert tokenize(\"ab..ab. .ab . ab.\") == [\"ab\", \"ab\", \"ab\", \"ab\"], tokenize(\"ab..ab. .ab . ab.\")\n",
    "    assert tokenize(\"word-of-mouth hello,world\")== [\"word\", \"of\", \"mouth\", \"hello\", \"world\"]\n",
    "    assert tokenize(\"gotta\")== [\"got\", \"ta\"]\n",
    "    assert tokenize(\"hello_world\") == [\"hello\", \"world\"]\n",
    "    assert preprocess_text(\"hope thisüëèwill work\") == [\"hope\", \"this\", \"will\", \"work\"]\n",
    "\n",
    "    # Lemmatize text.\n",
    "    assert lemmatize([\"cats\"]) == [\"cat\"]\n",
    "    assert lemmatize([\"did\"]) == [\"do\"]\n",
    "    assert lemmatize([\"learning\", \"is\", \"fun\"], english_stopwords) == [\"learn\", \"fun\"]\n",
    "\n",
    "    # Preprocess text.\n",
    "    assert preprocess_text(\"the weather is really nice\", english_stopwords) == [\"weather\", \"really\", \"nice\"]\n",
    "    assert preprocess_text(\n",
    "        \"To apply SVM learning in partial discharge classification, data input is very important!?\",\n",
    "        english_stopwords\n",
    "    ) == \"apply svm learn partial discharge classification data input important\".split()\n",
    "    assert preprocess_text(\"after all he's done\", english_stopwords) == []\n",
    "    assert preprocess_text(\"they didn‚Äôt have much chance of guessing what it was without further clues.\", english_stopwords) == [\"much\", \"chance\", \"guess\", \"without\", \"far\", \"clue\"]\n",
    "    assert preprocess_text(\"DUQUE'S\", english_stopwords) == [\"duque\"]\n",
    "    assert preprocess_text(\"the 'rona\", english_stopwords) == [\"rona\"]\n",
    "    assert preprocess_text('MOTORCYCLES DONT FLY', english_stopwords)==[\"motorcycle\", \"dont\", \"fly\"]\n",
    "    assert preprocess_text('‚Äú Georg e\\‚Äù', english_stopwords) == [\"georg\"]\n",
    "    text = \"Harry leapt into the air; he‚Äôd trodden on something big and squashy on the doormat ‚Äî something alive\"\n",
    "    assert preprocess_text(text, english_stopwords) == [\"harry\", \"leapt\", \"air\", \"trodden\", \"something\", \"big\", \"squashy\", \"doormat\", \"something\", \"alive\"]\n",
    "    assert preprocess_text(\"Don√¢‚Ç¨‚Ñ¢t want to add to TRUMP√¢‚Ç¨‚Ñ¢s #COVID19 numbers. #CoronaVirus √∞≈∏¬¶  don√¢‚Ç¨‚Ñ¢t care.\", english_stopwords) == [\"want\", \"add\", \"trump\", \"covid19\", \"number\", \"coronavirus\", \"care\"]\n",
    "\n",
    "    # Test a long string.\n",
    "    with open(\"local_test_refs/henrys_letter.txt\", encoding = \"utf-8\") as infile, open(\"local_test_refs/processed_henrys_letter.txt\", encoding = \"utf-8\") as outfile:\n",
    "        processed_str = preprocess_text(infile.read())\n",
    "        reference_str = outfile.read().splitlines()\n",
    "        assert processed_str == reference_str\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_preprocess_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bgvLcWF0wrH"
   },
   "source": [
    "You may notice that the lemmatization functionality isn't perfect; for example, it would map `\"as\"` to `\"a\"` because `\"as\"` is being treated as a noun instead of a proposition (with tag `\"IN\"`). In general, identifying the correct part-of-speech tag is very context-dependent (for example, `\"back\"` can be either an adjective, adverb, verb or noun). In the context of this project, we will not dive deep into these linguistic nuances, and settle with the lemmatization rules above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nvfjaodx0wrX"
   },
   "source": [
    "## Part E: Data Visualization and Feature Construction\n",
    "Now that we have collected text data from three different sources (Twitter, news articles and research papers), let's put them all together in order to perform some simple exploratory data analyses and feature construction. From now we will define a *document* as a list of tokens coming from a single tweet, news article or arxiv paper, and a *corpus* as a list of documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOcbOIhY0wrX"
   },
   "source": [
    "### Question 7: Word frequency and word cloud\n",
    "With any text corpus, you will first want to check for the word frequency distribution, in particular which words are the most common and which are the least. The former group may consist of terms that are relevant to the topic, or terms that simply appear frequently in general (e.g., stopwords). The latter group may consist of highly specialized terms or typos. Since stopwords and rare words are not useful to our analysis, we will remove both (where we define rare words as words that only appear *once in the corpus*).\n",
    "\n",
    "Implement the function `word_frequency` which takes as input a text corpus and returns a `collections.Counter` object mapping each word to its frequency in the corpus. However, rare words that only appear once in the entire corpus should **not** be included in this mapping.\n",
    "\n",
    "**Notes**:\n",
    "* Recall that `preprocess_text` already handles stopword removal, so you only need to remove rare words in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "JoATsTY90wrX"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def word_frequency(corpus: List[List[str]]) -> collections.Counter:\n",
    "    \"\"\"\n",
    "    Count the word frequency in a given corpus.\n",
    "\n",
    "    args:\n",
    "        corpus (List[List[str]]) : a nested list of tokens, where each inner list is a processed document\n",
    "\n",
    "    return:\n",
    "        collections.Counter : a mapping between each word and its frequency in the corpus, excluding words that\n",
    "            only appear once\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for document in corpus:\n",
    "        counter.update(document)\n",
    "    counter = Counter({item: count for item, count in counter.items() if count > 1})\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xg4a2b-60wrY",
    "outputId": "b9e36ead-823e-4285-b5f2-c5b742548969",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_word_frequency():\n",
    "    \"\"\"\n",
    "    Test the word_frequency function on a sample tweet corpus.\n",
    "\n",
    "    This function processes the first 100 tweets from the tweet_data, \n",
    "    preprocesses the text by removing stopwords, and then calculates \n",
    "    the word frequency using the word_frequency function. \n",
    "    Several assertions are made to ensure the correctness of the function.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    tweet_corpus = [\n",
    "        preprocess_text(tweet, english_stopwords)\n",
    "        for tweet in process_tweet_data(twitter_dataset)[:100]\n",
    "    ]\n",
    "    counter = word_frequency(tweet_corpus)\n",
    "    assert len(counter) == 230\n",
    "    assert counter[\"coronavirus\"] == 37\n",
    "    assert counter[\"coronavirusoutbreak\"] == 74\n",
    "    assert counter.get(\"the\",0) == 0\n",
    "    assert counter[\"say\"] == 4\n",
    "    assert min(counter.values()) == 2\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_word_frequency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76yj4IBo0wrY"
   },
   "source": [
    "Now we will gather all three corpora together; we store them in a global cache to avoid having to construct them more than once. If you make any code change above this point, rerun the following cell to reset the cache. Note that this will take around 10 minutes to run, and that we've purposefully excluded the cell that runs this from the autograder.\n",
    "\n",
    "**IMPORTANT NOTE**: For grading the functions after this point, we will cache 'local_corpus_store.pkl', which is just a simple file that contains all of the data processed through the functions you've written already. If you change anything above, you will need to re-run this cell in order to ensure that grading works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51Vd52oq0wrY",
    "outputId": "af8b1be6-d6d4-4aa5-b5df-e76187190f7f"
   },
   "outputs": [],
   "source": [
    "corpuses = None\n",
    "\n",
    "def get_corpuses():\n",
    "    \"\"\"\n",
    "    Retrieves or generates three different corpora (Twitter, News, and ArXiv) by processing \n",
    "    tweet data, news articles, and arXiv PDFs. The corpora are preprocessed by tokenizing \n",
    "    the text and removing stopwords.\n",
    "\n",
    "    If the corpuses have been previously generated and stored in the global variable \n",
    "    `corpuses`, the function will return the existing corpora. If not, it will generate them \n",
    "    from scratch, store them in `corpuses`, and then return them.\n",
    "\n",
    "    The corpora are generated by processing:\n",
    "        1. **Twitter Corpus:** The first 250 tweets are processed from a dataset and tokenized.\n",
    "        2. **News Corpus:** The first 200 news articles are parsed from HTML files and tokenized.\n",
    "        3. **ArXiv Corpus:** The first 50 arXiv PDFs are processed and tokenized.\n",
    "\n",
    "    Each corpus is preprocessed by the `preprocess_text` function, which handles \n",
    "    tokenization and the removal of English stopwords.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists of preprocessed text data:\n",
    "            - The first list contains the processed Twitter data.\n",
    "            - The second list contains the processed news article data.\n",
    "            - The third list contains the processed ArXiv paper data.\n",
    "    \"\"\"\n",
    "    global corpuses\n",
    "\n",
    "    if corpuses is None:\n",
    "        twitter_corpus = [\n",
    "            preprocess_text(elem, english_stopwords)\n",
    "            for elem in process_tweets(\n",
    "                tqdm(twitter_dataset[:250])\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        news_corpus = [\n",
    "            preprocess_text(elem, english_stopwords)\n",
    "            for elem in tqdm(\n",
    "                process_news_articles_data([\n",
    "                    parse_page_nature(f\"html_data/nature_{digit}.html\")\n",
    "                    for digit in range(200)\n",
    "                ])\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        arxiv_corpus = [\n",
    "            preprocess_text(elem, english_stopwords)\n",
    "            for elem in tqdm(\n",
    "                process_arxiv_data(\"pdfs\", num_files=50)\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        corpuses = (twitter_corpus, news_corpus, arxiv_corpus)\n",
    "\n",
    "    return corpuses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[148]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlocal_corpus_store.pkl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     pickle.dump(\u001b[43mget_corpuses\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, file)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[146]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mget_corpuses\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m corpuses\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m corpuses \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     30\u001b[39m     twitter_corpus = [\n\u001b[32m     31\u001b[39m         preprocess_text(elem, english_stopwords)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m \u001b[43mprocess_tweets\u001b[49m(\n\u001b[32m     33\u001b[39m             tqdm(twitter_dataset[:\u001b[32m250\u001b[39m])\n\u001b[32m     34\u001b[39m         )\n\u001b[32m     35\u001b[39m     ]\n\u001b[32m     37\u001b[39m     news_corpus = [\n\u001b[32m     38\u001b[39m         preprocess_text(elem, english_stopwords)\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m         )\n\u001b[32m     45\u001b[39m     ]\n\u001b[32m     47\u001b[39m     arxiv_corpus = [\n\u001b[32m     48\u001b[39m         preprocess_text(elem, english_stopwords)\n\u001b[32m     49\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[32m     50\u001b[39m             process_arxiv_data(\u001b[33m\"\u001b[39m\u001b[33mpdfs\u001b[39m\u001b[33m\"\u001b[39m, num_files=\u001b[32m50\u001b[39m)\n\u001b[32m     51\u001b[39m         )\n\u001b[32m     52\u001b[39m     ]\n",
      "\u001b[31mNameError\u001b[39m: name 'process_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"local_corpus_store.pkl\", \"wb\") as file:\n",
    "    pickle.dump(get_corpuses(), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRtZmxAc0wrY"
   },
   "source": [
    "Let's first compare the frequency of a number of keywords across these three corpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 895
    },
    "id": "s6Xi4zBf0wrY",
    "outputId": "135f254f-aeb3-4ed2-d501-214c93f33ae8",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell has been taggged with excluded_from_script\n",
    "# it will not be run by the autograder\n",
    "def get_word_frequency_across_corpuses(input_words):\n",
    "    twitter_corpus, news_corpus, arxiv_corpus = get_corpuses()\n",
    "    twitter_corpus_size = sum(len(d) for d in twitter_corpus)\n",
    "    news_corpus_size = sum(len(d) for d in news_corpus)\n",
    "    arxiv_corpus_size = sum(len(d) for d in arxiv_corpus)\n",
    "    twitter_f, news_f, arxiv_f = word_frequency(twitter_corpus), word_frequency(news_corpus), word_frequency(arxiv_corpus)\n",
    "    return pd.DataFrame({\n",
    "        \"Proportion in twitter corpus\" : [twitter_f.get(word, 0) / twitter_corpus_size for word in input_words],\n",
    "        \"Proportion in news corpus\" : [news_f.get(word, 0) / news_corpus_size for word in input_words],\n",
    "        \"Proportion in arxiv corpus\" : [arxiv_f.get(word, 0) / arxiv_corpus_size for word in input_words]\n",
    "    }, index = input_words)\n",
    "\n",
    "df_frequency = get_word_frequency_across_corpuses([\n",
    "    \"coronavirus\", \"covid\", \"case\", \"health\", \"model\", \"say\", \"test\",\n",
    "    \"2020\", \"19\", \"people\", \"vaccine\"\n",
    "])\n",
    "\n",
    "display(df_frequency)\n",
    "\n",
    "df_frequency.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNj82vLk0wrY"
   },
   "source": [
    "We see that there are differences across datasets in the relative frequency of each term. \"Coronavirus\" is used most frequently in tweets, \"say\" most frequently in news corpus, and perhaps unsurprisingly, \"model\" most frequently in arxiv papers. The scientific notation of coronavirus, \"covid,\" isn't used in news articles as much, but is equally popular in both tweets and arxiv papers. On the other hand, \"health\" sees most frequent usage in news articles, likely due to health advice-related articles. Feel free to edit the word list above and see what other insights you can derive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJlxkJq30wrZ"
   },
   "source": [
    "We now move to the last step of data collection and preparation: constructing input features to be used for more formal analyses and language modeling. As language modeling will be covered later in the course, here we will only cover two simple feature construction methods: term frequency (TF) and term frequency - inverse document frequency (TF-IDF), and then use them for our initial task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD4rdRII0wrZ"
   },
   "source": [
    "### Feature construction: term frequency (TF)\n",
    "Implement the function `construct_tf_matrix` that takes as input a corpus and outputs a matrix $TF$ where each row corresponds to one document, and each column corresponds to one of the unique words in the entire corpus. $TF_{ij}$ is the number of times word $j$ appears in document $i$. Similar to the previous question, rare words that only appear once in the entire corpus should be removed, i.e., there should be no columns for those words.\n",
    "\n",
    "**Notes**:\n",
    "* The rows should be ordered based on the document ordering in the corpus. Row 0 corresponds to `corpus[0]`, row 1 to `corpus[1]`, and so on.\n",
    "* The columns should be ordered based on the alphabetical order of their corresponding words. Column 0 corresponds to the alphabetically first word in the corpus, column 1 to the alphabetically second word, and so on.\n",
    "* To ensure code efficiency, avoid using too many loops. Take advantage of Pandas and Numpy functionalities.\n",
    "* We expect you to return an int64 as a datatype. Using `.astype(np.int64)` will help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "eUJsq5390wrZ"
   },
   "outputs": [],
   "source": [
    "def construct_tf_matrix(corpus):\n",
    "    \"\"\"\n",
    "    Construct a term frequency matrix from an input corpus\n",
    "\n",
    "    args:\n",
    "        corpus (List[List[str]]) : a nested list of word tokens, where each inner list is a document\n",
    "\n",
    "    return:\n",
    "        np.array[n_documents, n_words] : the term frequency matrix\n",
    "    \"\"\"\n",
    "    counters = [collections.Counter(doc) for doc in corpus]\n",
    "    df = pd.DataFrame(counters).fillna(0)\n",
    "    return df.loc[:, df.sum(axis = 0) > 1].sort_index(axis = 1).to_numpy(dtype = np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7y6ISpr0wrZ"
   },
   "source": [
    "### Feature construction: term frequency - inverse document frequency (TF-IDF)\n",
    "We can now compute the TF-IDF matrix, which scales the columns of the term frequency matrix by their inverse document frequency. Recall that the inverse document frequency of a word $j$ is computed as\n",
    "$$\\text{IDF}_j = \\log \\left( \\frac{\\# \\text{ of documents}}{\\# \\text{ of documents with word } j} \\right),$$\n",
    "and so the $\\text{TF-IDF}_{ij}$ entry in the tf-idf matrix is computed as\n",
    "$$\\text{TF-IDF}_{ij} = \\text{TF}_{ij} \\times \\text{IDF}_j.$$\n",
    "\n",
    "Implement the function `tf_idf_matrix` which takes as input a TF matrix and outputs the corresponding TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Pc2KXaiM0wra"
   },
   "outputs": [],
   "source": [
    "def construct_tf_idf_matrix(tf_matrix):\n",
    "    \"\"\"\n",
    "    Compute the term frequency - inverse document frequency in a corpus\n",
    "\n",
    "    args:\n",
    "        tf_matrix (np.array[n_documents, n_words]) : the term frequency document of the corpus\n",
    "\n",
    "    return:\n",
    "        np.array[n_documents, n_words] : the tf-idf matrix\n",
    "    \"\"\"\n",
    "    idf_matrix = np.log(tf_matrix.shape[0] / np.count_nonzero(tf_matrix, axis=0))\n",
    "    return tf_matrix.astype(np.float64) * idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moixUkGrjdJ4"
   },
   "source": [
    "## Dataset Similarity Comparison\n",
    "\n",
    "Now that we have two separate feature construction pipelines, let's evaluate how similar all three of our datasets are in the lense of these feature construction pipelines. To do so, we shall implement a **very** simple model-based metric called PAD.\n",
    "\n",
    "The idea behind PAD is very simple:\n",
    "\n",
    "1. Train a classification model to try to predict the dataset given the prediction scheme.\n",
    "2. Report the classification error on some dataset, **e**\n",
    "3. Compute **2*(1-2e)** as the metric itself.\n",
    "\n",
    "For our classification model, we shall have you implement a simple logistic regression based classifier. It is important to note that, traditionally, this approach uses a different model called a \"Support Vector Machine\" instead.\n",
    "\n",
    "### Logistic Regression-Based Classification\n",
    "\n",
    "Recall from the primer that logistic regression assumes the following hypothesis function:\n",
    "$$h_\\theta(x) = \\sigma(b + \\theta^T x)$$\n",
    "where $\\sigma(z) = (1+e^{-z})^{-1}$ is the sigmoid function.\n",
    "\n",
    "With this hypothesis funtion, input data $X \\in \\mathbb{R}^{n \\times d}$ and output labels $Y \\in \\{0,1\\}^{n}$, logistic regression attempts to minimize the loss function\n",
    "$$\\mathcal{L}(\\theta, b) = -\\frac{1}{2n} \\left[{\\sum_{i=1}^{n}} y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] + \\frac{\\lambda}{2n} \\|\\theta\\|_2^2,$$\n",
    "where $\\theta \\in \\mathbb{R}^{d}$ is the weight, $b$ is the intercept, and $\\lambda \\ge 0$ is the regularization parameter.\n",
    "\n",
    "This optimization can be carried out by gradient descent. Given a learning rate $\\alpha$, batch gradient descent for training logistic regression consists of two steps:\n",
    "\n",
    "1. Initialize $b = 0$ and $\\theta$ as a vector of 0s.\n",
    "1. Repeat `n_iters` times:\n",
    "\n",
    "\\begin{align}\n",
    "    b & := b - \\alpha \\cdot  \\frac{1}{2n} \\sum_{i=1}^n \\left(h_\\theta(x^{(i)}) - y^{(i)} \\right), \\\\\n",
    "    \\theta & := \\theta - \\alpha \\cdot \\frac{1}{2n} \\cdot \\left[\\sum_{i=1}^n (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} + 2\\lambda \\theta \\right]\n",
    "\\end{align}\n",
    "\n",
    "After training, we can predict the label for a new data point $x$ as\n",
    "$$\\hat y = \\mathbb{1}\\left(h_\\theta(x) \\ge \\frac 1 2 \\right) = \\mathbb{1}(b + \\theta^T x \\ge 0).$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Implement the class `LRClassifier` with 6 methods -- `__init__`, `loss`, `fit`, `get_weights`, `decision_function` and `predict` -- to perform the above tasks. You can create instance variables as you see fit.\n",
    "\n",
    "**Notes**:\n",
    "* A `LRClassifier` instance may be created once and then trained on several datasets. Therefore, you should initialize $b$ and $\\theta$ inside `.fit`, not in `__init__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "45rNELuzpjCI"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LRClassifier:\n",
    "    def __init__(self, lam):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "\n",
    "        args:\n",
    "            lam (float) : the regularizer value\n",
    "        \"\"\"\n",
    "        self.lam = lam\n",
    "\n",
    "    def loss(self, h, y):\n",
    "        \"\"\"\n",
    "        Compute the average loss L(theta, b) based on the provided formula.\n",
    "\n",
    "        args:\n",
    "            h (np.array[n_samples]) : a vector of hypothesis function values on every input data point,\n",
    "                this is the output of self.decision_function(X)\n",
    "            y (np.array[n_samples]) : the output label vector, containing 0 and 1\n",
    "\n",
    "        return:\n",
    "            np.float64 : the average loss value\n",
    "        \"\"\"\n",
    "        log_loss = np.mean(-y * np.log(h) - (1 - y) * np.log(1 - h)) / 2\n",
    "        regularizer = self.lam*(self.theta**2).sum()/(2*len(y))\n",
    "        return log_loss + regularizer\n",
    "\n",
    "    def fit(self, X, y, n_iters = 100, alpha = 1):\n",
    "        \"\"\"\n",
    "        Train the model weights and intercept term using batch gradient descent.\n",
    "\n",
    "        args:\n",
    "            X (np.array[n_samples, n_dimensions]) : the input data matrix\n",
    "            y (np.array[n_samples]) : the output label vector, containing 0 and 1\n",
    "\n",
    "        kwargs:\n",
    "            n_iters (int) : the number of iterations to train for\n",
    "            alpha (float) : the learning rate\n",
    "\n",
    "        return:\n",
    "            List[np.float64] : a list of length (n_iters + 1) that contains the loss value\n",
    "                before training and after each training iteration\n",
    "        \"\"\"\n",
    "        self.theta, self.b = np.zeros(X.shape[1]), 0\n",
    "        n = len(y)\n",
    "        losses = []\n",
    "        for _ in range(n_iters):\n",
    "            h = self.decision_function(X)\n",
    "            losses.append(self.loss(h, y))\n",
    "            theta_gradient = 1/(2*n) * ((h - y) @ X) + self.lam*self.theta/n\n",
    "            b_gradient = 1/2 * (h - y).mean()\n",
    "            self.theta -= alpha * theta_gradient\n",
    "            self.b -= alpha * b_gradient\n",
    "        h = self.decision_function(X)\n",
    "        losses.append(self.loss(h, y))\n",
    "        return losses\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Get the model weights and intercept term.\n",
    "\n",
    "        return:\n",
    "            Tuple(theta, b):\n",
    "                theta (np.array[n_dimensions]) : the weight vector\n",
    "                b (np.float64) : the intercept term\n",
    "        \"\"\"\n",
    "        return self.theta, self.b\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute the hypothesis function values on every input data point.\n",
    "\n",
    "        args:\n",
    "            X (np.array[n_samples, n_dimensions]) : the input data matrix\n",
    "\n",
    "        return:\n",
    "            np.array[n_samples] : a vector of hypothesis function values on every input data point\n",
    "        \"\"\"\n",
    "        return sigmoid(self.b + X @ self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the label of every input data point.\n",
    "\n",
    "        args:\n",
    "            X (np.array[n_samples, n_dimensions]) : the input data matrix\n",
    "\n",
    "        return:\n",
    "            np.array[n_samples] : a vector of predicted output labels for every input data point\n",
    "        \"\"\"\n",
    "        return np.where(self.b + X @ self.theta >= 0, 1, 0)\n",
    "\n",
    "def binary_lr_classifier(lam = 1e-4):\n",
    "    return LRClassifier(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eYmGoMvpjw9",
    "outputId": "b0643a7c-f42f-48dc-bd71-126fcd501c7c",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_binary_lr_classifier():\n",
    "    X = np.array([[-2, 4], [4, 1], [1, 6], [2, 4], [6, 2]])\n",
    "    y = np.array([0, 0, 1, 1, 1])\n",
    "    lr = binary_lr_classifier(lam = 1e-4)\n",
    "\n",
    "    # before gradient descent\n",
    "    losses = lr.fit(X, y, n_iters = 0)\n",
    "    theta, b = lr.get_params()\n",
    "    assert np.allclose(theta, [0, 0])\n",
    "    assert b == 0\n",
    "    assert np.allclose(losses[-1], 0.34657359027997264)\n",
    "    assert np.allclose(lr.decision_function(X), [0.5] * 5)\n",
    "    assert list(lr.predict(X)) == [1] * len(y)\n",
    "\n",
    "    # 1st iteration\n",
    "    losses = lr.fit(X, y, n_iters = 1)\n",
    "    theta, b = lr.get_params()\n",
    "    assert np.allclose(theta, [0.35, 0.35])\n",
    "    assert b == 0.05\n",
    "    assert np.allclose(losses[-1], 0.33351806318231178)\n",
    "    assert np.allclose(lr.decision_function(X), [0.6791786991753931, 0.8581489350995123, 0.9241418199787566, 0.8956687768809987, 0.9453186827840592])\n",
    "    assert list(lr.predict(X)) == [1] * len(y)\n",
    "\n",
    "    # 2 iterations\n",
    "    losses = lr.fit(X, y, n_iters = 2)\n",
    "    theta, b = lr.get_params()\n",
    "    assert np.allclose(theta, [0.20383002, 0.09069029])\n",
    "    assert np.allclose(b, -0.080246)\n",
    "    assert np.allclose(losses[-1], 0.28778446849618766)\n",
    "    assert np.allclose(lr.decision_function(X), [0.4687546229122032, 0.6954586477733905, 0.6609937974719609, 0.6660059656189242, 0.7898655199585818])\n",
    "    assert list(lr.predict(X)) == [0, 1, 1, 1, 1]\n",
    "\n",
    "    # 1000 iterations\n",
    "    losses = lr.fit(X, y, n_iters = 1000)\n",
    "    theta, b = lr.get_params()\n",
    "    assert np.allclose(theta, [1.62475335, 2.97699553])\n",
    "    assert np.allclose(b, -12.016701793625622)\n",
    "    assert np.allclose(losses[-1], 0.0178892651602277)\n",
    "    assert np.allclose(lr.decision_function(X), [0.0336268115487116, 0.07305423924580728, 0.9994304104089492, 0.9585441655688948, 0.9755365947084815])\n",
    "    assert list(lr.predict(X)) == [0, 0, 1, 1, 1]\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_binary_lr_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcRPn3gupr5z"
   },
   "source": [
    "## Proxy A-Distance Calculation\n",
    "\n",
    "Now to pull it all together. Let's implement a function ``calculate_pad_distance`` that does the following:\n",
    "\n",
    "1. Given a matrix of features ``train_X``, and a list of one-hot encoded binary data ``train_y``, train a logistic regression classifier on that data, setting lambda to be `1e-4`.\n",
    "2. Using a test matrix of features ``test_X`` and a list of test labels ``test_y``, calculate the classification error **e**. If the classification error is greater than 0.5, let the classification error be **1 - e** instead, as we can always flip our classifier's ratings.\n",
    "3. Return **2\\*(1-2e)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "z6wI2GvHprHO"
   },
   "outputs": [],
   "source": [
    "def calculate_pad_distance(train_X, train_y, test_X, test_y):\n",
    "  \"\"\"\n",
    "  Compute the Proxy A-Distance using the LRClassifier\n",
    "\n",
    "  return:\n",
    "      float:\n",
    "        The Proxy A-Distance, training on train_X and train_y and testing on test_X and test_y\n",
    "  \"\"\"\n",
    "  model = LRClassifier(1e-4)\n",
    "  model.fit(train_X, train_y)\n",
    "  error = min(np.mean(model.predict(test_X) == test_y), np.mean(model.predict(test_X) != test_y))\n",
    "  return 2 * (1 - 2*error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4rLGmQGC0wg"
   },
   "source": [
    "With that computed, let's now calculate the PAD between all three of our data sources.\n",
    "\n",
    "In this case, we shall evaluate the PAD for each pair of datasets using just term frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqNSZ_aasl3R",
    "outputId": "bbb3af19-104e-45eb-8096-05962c7e2a4c",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "tf = construct_tf_matrix(corpuses[0] + corpuses[1] + corpuses[2])\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "train_X = np.concatenate([tf[:200, :],tf[200:350,:]])\n",
    "train_y = np.array([0]*200 + [1]*150)\n",
    "test_X = np.concatenate([tf[200:250, :],tf[350:400,:]])\n",
    "test_y = np.array([0]*50 + [1]*50)\n",
    "print(\"PAD using TF between Twitter and News:\", calculate_pad_distance(train_X, train_y, test_X, test_y))\n",
    "\n",
    "train_X = np.concatenate([tf[:200, :],tf[400:450,:]])\n",
    "train_y = np.array([0]*200 + [1]*50)\n",
    "test_X = np.concatenate([tf[200:250, :],tf[450:500,:]])\n",
    "test_y = np.array([0]*50 + [1]*50)\n",
    "print(\"PAD using TF between Twitter and Arxiv:\", calculate_pad_distance(train_X, train_y, test_X, test_y))\n",
    "\n",
    "train_X = np.concatenate([tf[200:350,:],tf[400:450,:]])\n",
    "train_y = np.array([0]*150 + [1]*50)\n",
    "test_X = np.concatenate([tf[350:400,:],tf[450:500,:]])\n",
    "test_y = np.array([0]*50 + [1]*50)\n",
    "print(\"PAD using TF between News and Arxiv:\", calculate_pad_distance(train_X, train_y, test_X, test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-9MPTzhEbYY"
   },
   "source": [
    "The PAD between Twitter and News is ``1.44``, the PAD between Twitter and Arxiv is ``1.64``, and the PAD between News and Arxiv is ``0.16``.\n",
    "\n",
    "Implicitly, this makes a lot of sense, as News and Arxiv text are largely more similar, as long-form text, compared with Twitter/X posts, which are limited in length.\n",
    "\n",
    "If we use tf-idf, however, we get a different picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJbAW65jE3_0",
    "outputId": "f4c6ca32-b885-4be3-d2ea-c3a4e40f3387",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "tf = construct_tf_idf_matrix(construct_tf_matrix(corpuses[0] + corpuses[1] + corpuses[2]))\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "train_X = np.concatenate([tf[:200, :],tf[200:350,:]])\n",
    "train_y = np.array([0]*200 + [1]*150)\n",
    "test_X = np.concatenate([tf[200:250, :],tf[350:400,:]])\n",
    "test_y = np.array([0]*50 + [1]*50)\n",
    "print(\"PAD using TF-IDF between Twitter and News:\", calculate_pad_distance(train_X, train_y, test_X, test_y))\n",
    "\n",
    "train_X = np.concatenate([tf[:200, :],tf[400:450,:]])\n",
    "train_y = np.array([0]*200 + [1]*50)\n",
    "test_X = np.concatenate([tf[200:250, :],tf[450:500,:]])\n",
    "test_y = np.array([0]*50 + [1]*50)\n",
    "print(\"PAD using TF-IDF between Twitter and Arxiv:\", calculate_pad_distance(train_X, train_y, test_X, test_y))\n",
    "\n",
    "train_X = np.concatenate([tf[200:350,:],tf[400:450,:]])\n",
    "train_y = np.array([0]*150 + [1]*50)\n",
    "test_X = np.concatenate([tf[350:400,:],tf[450:500,:]])\n",
    "test_y = np.array([0]*50 + [1]*50)\n",
    "print(\"PAD using TF-IDF between News and Arxiv:\", calculate_pad_distance(train_X, train_y, test_X, test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTYWRFGkFDaE"
   },
   "source": [
    "Again, the PAD between Twitter and News is ``0.08``, the PAD between Twitter and Arxiv is ``0.76``, and the PAD between News and Arxiv is ``0.28``.\n",
    "\n",
    "As we can see, the PAD is on average smaller when using TF-IDF compared to using TF. This suggests that it is harder to distinguish between the three data sources using TF-IDF, and thus it might be better to use when training a model if we want to ensure that classifier performance is the same between all three data sources."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "provenance": []
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "projectEnv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
